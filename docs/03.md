### **Lección 3: Procesamiento de Datos**  
**Objetivo**: Dominar las estructuras de datos fundamentales en Spark (DataFrames y RDDs) y optimizar su uso en Python para ETL, análisis y machine learning.

---

### **1. DataFrame**  
#### **Definición**  
Estructura tabular distribuida y optimizada, similar a un DataFrame de Pandas pero diseñada para big data. Organiza los datos en filas y columnas con un esquema definido.  

#### **Características Clave**  
- **Distribuido**: Los datos se particionan automáticamente en nodos del cluster.  
- **Optimizado**: Usa el **Catalyst Optimizer** para planificar consultas eficientes.  
- **Inmutabilidad**: Cada operación genera un nuevo DataFrame (no modifica el original).  

---

#### **Operaciones Básicas en PySpark**  
##### **Lectura de Datos**  
```python
# Leer desde CSV en DBFS
df = spark.read.csv("dbfs:/mnt/datos/ventas.csv", header=True, inferSchema=True)

# Leer desde Parquet (formato columnar optimizado)
df_parquet = spark.read.parquet("dbfs:/mnt/datos_transacciones/")
```

##### **Transformaciones Comunes**  
```python
from pyspark.sql.functions import col, when

# Filtrar y crear nuevas columnas
df_transformado = df \
    .filter(col("ventas") > 1000) \
    .withColumn("categoria", 
                when(col("producto").startswith("A"), "Electrónicos")
                .otherwise("Otros"))
```

##### **Acciones**  
```python
# Mostrar las primeras filas (acción que dispara procesamiento)
df_transformado.show(5)

# Contar filas (acción que requiere procesamiento total)
print("Total de registros:", df_transformado.count())
```

---

#### **Optimización de DataFrames**  
1. **Particionamiento**:  
   ```python
   # Reparticionar para paralelismo óptimo (ej: 1 partición por núcleo)
   df_repartido = df.repartition(16)
   ```
2. **Caché**:  
   ```python
   # Almacenar en memoria para reutilización frecuente
   df_transformado.cache()
   ```
3. **Evitar Shuffles**:  
   - Usar `join` con claves pre-particionadas.  
   - Preferir `transformaciones estrechas` (filter, map) sobre `amplias` (groupBy, distinct).  

---

### **2. RDD (Resilient Distributed Dataset)**  
#### **Definición**  
Estructura de bajo nivel que representa una colección inmutable de datos distribuidos. Antes de los DataFrames, era la base de Spark.  

#### **Cuándo Usar RDDs**  
- Procesamiento **no estructurado** (texto, imágenes).  
- Operaciones **personalizadas** que no se pueden expresar con APIs de DataFrame.  
- Integración con bibliotecas antiguas o código Scala/Java.  

---

#### **Ejemplos en PySpark**  
##### **Crear un RDD**  
```python
# Desde una lista de Python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5], numSlices=4)  # 4 particiones

# Desde un archivo de texto
rdd_texto = spark.sparkContext.textFile("dbfs:/mnt/logs/server_logs.txt")
```

##### **Transformaciones y Acciones**  
```python
# Mapear y filtrar
rdd_cuadrados = rdd.map(lambda x: x ** 2)
rdd_filtrado = rdd_cuadrados.filter(lambda x: x % 2 == 0)

# Reducción (acción)
suma_total = rdd_filtrado.reduce(lambda a, b: a + b)
print("Suma de pares al cuadrado:", suma_total)
```

---

#### **RDD vs DataFrame**  
| **Característica**       | **RDD**                          | **DataFrame**                      |  
|--------------------------|-----------------------------------|------------------------------------|  
| **Optimización**         | Manual                            | Automática (Catalyst)              |  
| **Esquema**              | No estructurado                  | Estructurado (tipos definidos)     |  
| **Rendimiento**          | Más lento (sin optimización)     | Más rápido (Tungsten engine)       |  
| **Uso en Python**        | Menos común (excepto casos específicos) | Recomendado para el 90% de casos |  

---

### **Caso Práctico: Procesamiento Híbrido (DataFrame + RDD)**  
**Escenario**: Limpiar un dataset de texto semi-estructurado y calcular métricas.  

```python
# Paso 1: Usar RDD para limpieza inicial
rdd_logs = spark.sparkContext.textFile("dbfs:/mnt/logs/raw_logs.txt")

# Filtrar líneas corruptas y extraer campos
rdd_limpio = rdd_logs \
    .filter(lambda linea: "ERROR" in linea) \
    .map(lambda linea: (linea.split("|")[0], linea.split("|")[2]))

# Paso 2: Convertir a DataFrame para análisis estructurado
from pyspark.sql.types import StringType, StructType, StructField

esquema = StructType([
    StructField("timestamp", StringType()),
    StructField("mensaje_error", StringType())
])

df_errores = rdd_limpio.toDF(esquema)

# Paso 3: Agrupar errores por tipo
from pyspark.sql.functions import count
df_agrupado = df_errores.groupBy("mensaje_error").agg(count("*").alias("total"))

# Mostrar resultados
df_agrupado.show(truncate=False)
```

**Salida**:  
```
+-----------------------------+-----+  
|mensaje_error                |total|  
+-----------------------------+-----+  
|Timeout conexión DB          |142  |  
|Autenticación fallida        |89   |  
+-----------------------------+-----+  
```

---

### **Mejores Prácticas**  
1. **Preferir DataFrames sobre RDDs** a menos que se necesite control de bajo nivel.  
2. **Evitar Colectar Datos al Driver**:  
   ```python
   # Incorrecto (riesgo de OOM)
   datos_local = df.collect()

   # Correcto (escribir en almacenamiento distribuido)
   df.write.parquet("dbfs:/resultados/")
   ```
3. **Monitorizar el Plan de Ejecución**:  
   ```python
   df.explain(mode="formatted")  # Ver el plan de consulta optimizado
   ```

---

### **Errores Comunes y Soluciones**  
- **Problema**: `OutOfMemoryError` en el driver.  
  **Causa**: Uso excesivo de `collect()` o `toPandas()`.  
  **Solución**: Usar `limit()` o procesamiento distribuido.  

- **Problema**: Operaciones lentas con `groupBy()`.  
  **Causa**: Shuffling excesivo.  
  **Solución**: Usar `bucketBy()` al guardar datos para pre-partitionar.  

---

### **Preguntas Frecuentes**  
**Q1**: ¿Cómo convertir un DataFrame a RDD?  
```python
rdd_desde_df = df.rdd  # Cada fila se convierte en un objeto Row
```

**Q2**: ¿Cuándo usar `map()` vs `mapPartitions()`?  
- `map()`: Aplica una función por fila (alto overhead).  
- `mapPartitions()`: Opera en particiones completas (mejor para conexiones a BD).  

---