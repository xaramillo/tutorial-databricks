### **Lección 5: APIs Clave**  
**Objetivo**: Dominar las APIs esenciales de PySpark y Databricks para manipulación de datos, machine learning y automatización.

---

### **1. PySpark API**  
#### **Definición**  
Interfaz de Python para Apache Spark, diseñada para trabajar con datos distribuidos. Incluye módulos para SQL, streaming, machine learning y procesamiento estructurado.  

#### **Módulos Principales**  
- **`pyspark.sql`**: Manipulación de DataFrames y consultas SQL.  
- **`pyspark.ml`**: Machine Learning con pipelines y algoritmos predefinidos.  
- **`pyspark.streaming`**: Procesamiento de datos en tiempo real.  

---

#### **Ejemplos Prácticos**  
##### **DataFrames y SQL**  
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

# Crear DataFrame desde una lista
data = [("Alice", 34), ("Bob", 45), ("Carla", 28)]
df = spark.createDataFrame(data, ["Nombre", "Edad"])

# Filtrar usando SQL
df.createOrReplaceTempView("personas")
resultado = spark.sql("SELECT * FROM personas WHERE Edad > 30")
resultado.show()
```

##### **Operaciones Avanzadas**  
```python
from pyspark.sql.functions import udf, col
from pyspark.sql.types import IntegerType

# UDF para categorizar edades
def categoria_edad(edad):
    return 1 if edad >= 30 else 0

udf_categoria = udf(categoria_edad, IntegerType())
df = df.withColumn("Categoria", udf_categoria(col("Edad")))
```

---

### **2. UDF (User-Defined Function)**  
#### **Definición**  
Función personalizada escrita en Python y registrada para ejecutarse en paralelo en los workers de Spark.  

#### **Tipos de UDFs**  
- **UDF Estándar**: Ejecuta código Python en cada fila (menos eficiente).  
- **Pandas UDF (Vectorizada)**: Opera en particiones completas usando Pandas (mayor rendimiento).  

---

#### **Ejemplo Comparativo**  
##### **UDF Estándar**  
```python
# Registro y uso de una UDF lenta
@udf(IntegerType())
def calcular_cuadrado(edad):
    return edad ** 2

df = df.withColumn("Edad_Cuadrado", calcular_cuadrado(col("Edad")))
```

##### **Pandas UDF (Optimizada)**  
```python
from pyspark.sql.functions import pandas_udf

# UDF vectorizada con Pandas
@pandas_udf(IntegerType())
def calcular_cuadrado_vec(edades: pd.Series) -> pd.Series:
    return edades ** 2

df = df.withColumn("Edad_Cuadrado", calcular_cuadrado_vec(col("Edad")))
```

**Rendimiento**:  
- Las Pandas UDFs son **~10-100x más rápidas** que las UDFs estándar al evitar la serialización fila por fila.  

---

### **3. Databricks Utilities (`dbutils`)**  
Conjunto de herramientas integradas para gestionar datos, secrets y workflows directamente desde Python.  

#### **Funcionalidades Clave**  
| **Módulo**       | **Uso**                                      | **Ejemplo**                                  |  
|-------------------|----------------------------------------------|----------------------------------------------|  
| **`fs`**          | Manipulación de archivos en DBFS.            | `dbutils.fs.cp("ruta_origen", "ruta_destino")` |  
| **`widgets`**     | Crear parámetros interactivos en notebooks.  | `dbutils.widgets.text("input", "default")`   |  
| **`secrets`**     | Acceder a contraseñas almacenadas de forma segura. | `dbutils.secrets.get(scope="mi_scope", key="api-key")` |  

---

#### **Caso de Uso: Widgets + Secrets**  
```python
# Paso 1: Crear un widget para entrada de usuario
dbutils.widgets.dropdown("entorno", "dev", ["dev", "prod"])

# Paso 2: Leer credenciales según entorno
entorno = dbutils.widgets.get("entorno")
api_key = dbutils.secrets.get(scope=f"scope_{entorno}", key="api_key")

# Paso 3: Usar la clave en una conexión (ej: API externa)
import requests
response = requests.get(
    url="https://api.example.com/data",
    headers={"Authorization": f"Bearer {api_key}"}
)
```

---

### **4. Streaming API (Structured Streaming)**  
#### **Definición**  
API para procesar flujos de datos en tiempo real usando la misma sintaxis que los DataFrames estáticos.  

---

#### **Ejemplo: Procesamiento en Tiempo Real**  
```python
from pyspark.sql.types import StructType, StringType, TimestampType

# Definir esquema de datos entrantes
esquema = StructType([
    StructField("id_transaccion", StringType()),
    StructField("monto", IntegerType()),
    StructField("timestamp", TimestampType())
])

# Leer stream desde un directorio en S3
stream_df = spark.readStream \
    .schema(esquema) \
    .parquet("s3a://bucket-transacciones/streaming/")

# Procesar: Filtrar transacciones mayores a $1000
stream_filtrado = stream_df.filter(col("monto") > 1000)

# Escribir resultados en consola (para debug)
query = stream_filtrado.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()  # Mantener el stream activo
```

---

### **5. Integración con MLflow**  
Plataforma para gestionar el ciclo de vida de modelos machine learning.  

#### **Flujo de Trabajo con Python**  
```python
import mlflow
from sklearn.ensemble import RandomForestClassifier

# Paso 1: Iniciar experimento
mlflow.set_experiment("/mi-experimento")

# Paso 2: Entrenar modelo y registrar métricas
with mlflow.start_run():
    modelo = RandomForestClassifier(n_estimators=100)
    modelo.fit(X_train, y_train)
    
    # Registrar parámetros y métricas
    mlflow.log_param("n_estimators", 100)
    mlflow.log_metric("accuracy", accuracy_score(y_test, modelo.predict(X_test)))
    
    # Guardar modelo
    mlflow.sklearn.log_model(modelo, "modelo_rf")
```

---

### **Caso Práctico: Pipeline de ML End-to-End**  
**Objetivo**: Predecir churn de clientes usando PySpark y MLflow.  

```python
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline

# Paso 1: Preparar datos
assembler = VectorAssembler(inputCols=["edad", "uso_mensual"], outputCol="features")

# Paso 2: Definir modelo
lr = LogisticRegression(featuresCol="features", labelCol="churn")

# Paso 3: Crear pipeline
pipeline = Pipeline(stages=[assembler, lr])

# Paso 4: Entrenar y registrar en MLflow
with mlflow.start_run():
    modelo = pipeline.fit(df_train)
    predictions = modelo.transform(df_test)
    
    # Calcular métricas
    accuracy = predictions.filter(col("churn") == col("prediction")).count() / df_test.count()
    
    # Registrar en MLflow
    mlflow.log_metric("accuracy", accuracy)
    mlflow.spark.log_model(modelo, "modelo_churn")
```

---

### **Mejores Prácticas**  
1. **Evitar UDFs siempre que sea posible**: Preferir funciones nativas de Spark (`pyspark.sql.functions`).  
2. **Usar Delta Lake para streaming**: Garantiza exactamente una vez (exactly-once) procesamiento.  
3. **Monitorear streams**: Usar la interfaz de Spark UI para ver el progreso de queries en tiempo real.  

---

### **Errores Comunes y Soluciones**  
- **Problema**: `PicklingError` al usar UDFs con objetos complejos.  
  **Causa**: Spark no puede serializar funciones que dependan de recursos externos (ej: conexiones a BD).  
  **Solución**: Usar `broadcast variables` o inicializar recursos dentro de la UDF.  

- **Problema**: Latencia alta en streaming.  
  **Causa**: Microbatches demasiado frecuentes.  
  **Solución**: Aumentar el intervalo de procesamiento (`.trigger(processingTime='5 minutes')`).  

---

### **Preguntas Frecuentes**  
**Q1**: ¿Cómo usar bibliotecas externas en UDFs?  
- Instalar la librería en todos los nodos del cluster (usando `%pip install` en el notebook o cluster-scoped init scripts).  

**Q2**: ¿Cómo listar todos los experimentos de MLflow?  
```python
experimentos = mlflow.search_experiments()
for exp in experimentos:
    print(exp.name)
```

---

### **Resumen Integrado**  
```python
# Flujo típico con APIs clave:
dbutils.fs.mount(...)                # 1. Montar datos
df = spark.read.format(...)          # 2. Leer datos
udf_personalizada(...)               # 3. Transformar
modelo = entrenar_con_mlflow(...)    # 4. Machine Learning
modelo.save("dbfs:/modelos")         # 5. Guardar resultados
```

---