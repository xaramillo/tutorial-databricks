### **Lección 1: Conceptos Básicos**  
**Objetivo**: Entender los fundamentos de Databricks y su integración con Apache Spark.

---

#### **1. Apache Spark**  
**Definición**:  
Motor de procesamiento de datos distribuido diseñado para big data. Es la base de Databricks y permite procesar grandes volúmenes de datos en paralelo.  

**Relevancia en Databricks**:  
- Se usa a través de la API `pyspark` en Python.  
- Permite ejecutar operaciones ETL, machine learning y análisis en clusters.  

**Ejemplo en Python**:  
```python
# Inicializar una sesión de Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MiPrimerSpark").getOrCreate()

# Cargar datos desde un archivo CSV
df = spark.read.csv("dbfs:/datos_ejemplo.csv", header=True)
```

---

#### **2. Databricks Workspace**  
**Definición**:  
Entorno colaborativo en la nube donde se gestionan notebooks, bibliotecas, datos y clusters.  

**Relevancia en Databricks**:  
- Organiza proyectos en carpetas y repositorios.  
- Integra herramientas como Git, MLflow y dashboards.  

**Componentes clave**:  
- **Notebooks**: Para escribir código (Python, SQL, Scala).  
- **Repos**: Para clonar repositorios de Git.  
- **Clusters UI**: Para configurar recursos computacionales.  

**Ejemplo práctico**:  
```python
# Usar utilidades de Databricks (dbutils) para navegar en el workspace
archivos = dbutils.fs.ls("/mnt/data")
for archivo in archivos:
    print(archivo.name)
```

---

#### **3. Notebook**  
**Definición**:  
Documento interactivo que combina código ejecutable (Python, SQL), texto explicativo (Markdown/HTML) y visualizaciones.  

**Relevancia en Databricks**:  
- Ideal para prototipado rápido y colaboración.  
- Soporta ejecución celda por celda.  

**Características**:  
- **Celdas de código**: Ejecutan código Python con Spark.  
- **Celdas Markdown**: Documentan el flujo de trabajo.  
- **Magic Commands**: Comandos especiales como `%sql` para cambiar lenguaje.  

**Ejemplo en Python**:  
```python
# Celda 1: Calcular estadísticas básicas
df.show(5)

# Celda 2: Visualizar datos
display(df.groupBy("categoria").count())
```

```markdown
# Celda Markdown de ejemplo:
**Nota**: Este notebook procesa datos de ventas mensuales.
```

---

### **Resumen Práctico**  
```python
# Combinar los tres conceptos en un flujo básico:
# 1. Usar Spark para leer datos
df = spark.read.csv("dbfs:/datos.csv", inferSchema=True, header=True)

# 2. Explorar datos desde el notebook
print("Número de filas:", df.count())
display(df.describe())

# 3. Guardar resultados en DBFS (Databricks File System)
df.write.parquet("dbfs:/resultados.parquet")
```

---