### **Lección 4: Almacenamiento**  
**Objetivo**: Dominar el uso del sistema de archivos de Databricks (DBFS), formatos de almacenamiento optimizados y estrategias para integrar datos locales/remotos.

---

### **1. DBFS (Databricks File System)**  
#### **Definición**  
Sistema de archivos virtual que abstrae almacenamientos externos (S3, ADLS, GCS) y locales. Funciona como una capa unificada para acceder a datos desde notebooks y jobs.  

#### **Características Clave**  
- **Montaje de almacenamiento**: Conexión segura a servicios cloud (ej: S3 → `/mnt/bucket_s3`).  
- **Persistencia**: Los datos en DBFS sobreviven al reinicio de clusters.  
- **Acceso multiplataforma**: Compatible con APIs de Spark, Python y comandos Unix-like.  

---

#### **Estructura de Directorios**  
```bash
# Rutas comunes en DBFS:
/FileStore    # Archivos subidos manualmente (ej: CSVs, JSONs)
/mnt          # Puntos de montaje de almacenamientos externos
/tmp          # Datos temporales (se eliminan tras finalizar la sesión)
```

---

#### **Operaciones Básicas con `dbutils` (Python)**  
```python
# Listar archivos en DBFS
dbutils.fs.ls("/mnt/data_lake")

# Copiar datos desde local a DBFS
dbutils.fs.put("/FileStore/raw_data.csv", "contenido,en,CSV")

# Montar un bucket de S3 (requiere credenciales)
dbutils.fs.mount(
    source="s3a://mi-bucket-s3",
    mount_point="/mnt/s3_data",
    extra_configs={"aws_access_key_id": "XXX", "aws_secret_access_key": "YYY"}
)
```

---

### **2. Formatos de Almacenamiento**  
#### **Parquet**  
- **Ventajas**:  
  - Formato columnar (optimizado para consultas analíticas).  
  - Compresión eficiente (Snappy, GZIP).  
  - Soporta esquemas evolutivos.  

**Ejemplo en PySpark**:  
```python
# Leer/Escribir en Parquet
df = spark.read.parquet("/mnt/data_lake/transacciones.parquet")
df.write.parquet("/mnt/data_lake/transacciones_procesadas.parquet")
```

---

#### **Delta Lake**  
- **Ventajas**:  
  - Transacciones ACID (atomicidad, consistencia).  
  - Historial de versiones (time travel).  
  - Integración nativa con Spark.  

**Ejemplo en Python**:  
```python
# Convertir Parquet a Delta Lake
from delta.tables import DeltaTable

df.write.format("delta").save("/mnt/delta_lake/ventas")

# Time travel: Leer versión anterior
df_v1 = spark.read.format("delta").option("versionAsOf", 1).load("/mnt/delta_lake/ventas")
```

---

#### **CSV vs JSON vs Avro**  
| **Formato** | **Uso Recomendado**          | **Limitaciones**                |  
|-------------|-------------------------------|----------------------------------|  
| CSV         | Intercambio simple            | Sin soporte para tipos complejos |  
| JSON        | Datos semi-estructurados      | Rendimiento bajo en consultas   |  
| Avro        | Serialización binaria         | Menos integración con herramientas BI |  

---

### **3. Integración con Almacenamiento Cloud**  
#### **Montaje de AWS S3**  
```python
# Configurar montaje seguro (mejor práctica: usar IAM Roles)
dbutils.fs.mount(
    source="s3a://mi-bucket-raw",
    mount_point="/mnt/raw_data",
    extra_configs={
        "fs.s3a.aws.credentials.provider": "com.amazonaws.auth.InstanceProfileCredentialsProvider"
    }
)

# Leer datos directamente desde S3
df_s3 = spark.read.csv("s3a://mi-bucket-raw/ventas_2023.csv", header=True)
```

---

#### **Acceso a Azure Data Lake Storage (ADLS Gen2)**  
```python
# Configurar acceso usando Service Principal
spark.conf.set("fs.azure.account.auth.type.mi_storage.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.mi_storage.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.mi_storage.dfs.core.windows.net", "<client-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.mi_storage.dfs.core.windows.net", "<client-secret>")
spark.conf.set("fs.azure.account.oauth2.client.endpoint.mi_storage.dfs.core.windows.net", "https://login.microsoftonline.com/<tenant-id>/oauth2/token")

# Leer datos
df_adls = spark.read.parquet("abfss://container@mi_storage.dfs.core.windows.net/ruta/datos.parquet")
```

---

### **4. Caso Práctico: Pipeline ETL**  
**Objetivo**: Ingresar datos desde S3, transformarlos y guardar en Delta Lake.  

```python
# Paso 1: Leer datos crudos desde S3
df = spark.read.csv("/mnt/s3_data/raw/ventas.csv", header=True)

# Paso 2: Transformar datos
from pyspark.sql.functions import col, to_date
df_transformado = df \
    .withColumn("fecha_venta", to_date(col("timestamp"), "yyyy-MM-dd")) \
    .drop("timestamp") \
    .filter(col("monto") > 0)

# Paso 3: Guardar en Delta Lake con particionamiento
df_transformado.write \
    .format("delta") \
    .partitionBy("fecha_venta") \
    .mode("overwrite") \
    .save("/mnt/delta_lake/ventas_etl")

# Paso 4: Optimizar el archivo Delta
delta_table = DeltaTable.forPath(spark, "/mnt/delta_lake/ventas_etl")
delta_table.optimize().executeCompaction()  # Mejorar rendimiento de consultas
```

---

### **Mejores Prácticas**  
1. **Evitar archivos pequeños**:  
   ```python
   # Usar coalesce para reducir particiones
   df.coalesce(10).write.parquet(...)
   ```  
2. **Cifrado de datos**:  
   - Habilitar SSE (Server-Side Encryption) en S3/ADLS.  
   - Usar credenciales temporales (IAM Roles, SAS Tokens).  
3. **Gestión de versiones**:  
   - Usar Delta Lake para auditoría (`DESCRIBE HISTORY`).  

---

### **Errores Comunes y Soluciones**  
- **Problema**: `FileNotFoundException` al acceder a DBFS.  
  **Causa**: Rutas incorrectas o permisos insuficientes.  
  **Solución**: Verificar montajes con `dbutils.fs.ls("/mnt")`.  

- **Problema**: Bajo rendimiento en lecturas.  
  **Causa**: Formato inapropiado (ej: CSV con 10 GB).  
  **Solución**: Convertir a Parquet/Delta Lake y particionar.  

---

### **Preguntas Frecuentes**  
**Q1**: ¿Cómo compartir datos entre clusters?  
- Usar DBFS o almacenamiento cloud montado (ej: `/mnt/shared_data`).  

**Q2**: ¿Qué es el "Auto Loader" de Databricks?  
- Herramienta para ingesta continua de datos (ej: nuevos archivos en S3).  
  ```python
  df_stream = spark.readStream.format("cloudFiles") \
      .option("cloudFiles.format", "json") \
      .load("/mnt/s3_data/streaming/")
  ```

---

### **Resumen Visual**  
```python
# Flujo de trabajo típico:
dbutils.fs.mount(...)            # 1. Montar almacenamiento
df = spark.read.format(...)      # 2. Leer datos
df.transform(...)                # 3. Procesar
df.write.format("delta").save(...)  # 4. Guardar en formato óptimo
```

---